IPFS push/pull is implemented but completely disconnected. ipfs_remote.py has a full working implementation — it bundles all reachable objects into a binary blob, POSTs it to an IPFS gateway's /api/v0/add endpoint, and can pull bundles back via /ipfs/<cid>. But remote.py never calls any of it. There is no ipfs:// URL handling in the push or pull routing. The functions exist but no command ever reaches them. You need to add an ipfs:// branch in remote.py's push and pull methods that routes to push_to_ipfs and pull_from_ipfs. And even once routed, there's no pinning — pushed data will be garbage collected by IPFS nodes unless you pin it separately.
The compression pipeline is dead code. compression_pipeline.py is the only entirely new file in 0.1.6 and nothing imports it. No command calls it. The Distiller and Gardener don't use it. It has sentence-boundary chunking, fact extraction, hash-based dedup, similarity-based dedup, and hot/cold tiering logic — all reasonable — but none of it runs. It needs to be wired into the Distiller as a pre-processing stage before LLM fact extraction, or exposed as a standalone agmem compress command.
Differential privacy is applied to the wrong things. What's noised right now: the confidence_score in frontmatter, the source_episodes count in frontmatter, and the result summary counts (clusters processed, facts extracted, episodes archived). These are aggregate metadata. The actual extracted facts — the semantic content that ends up in the consolidated files — are completely un-noised. That's the opposite of what differential privacy is for here. The point is that the output facts should be differentially private with respect to any individual episodic log. If you remove one episode from the input and re-run distillation, the resulting semantic facts should be statistically indistinguishable. Noising the count of how many clusters were processed doesn't achieve that. The noise needs to be applied during fact extraction itself — either by using the LLM with a DP-constrained prompt (limiting how much any single episode can influence the output) or by adding noise to the extracted fact embeddings before consolidation.
The ZK proofs work but aren't zero-knowledge. The keyword containment proof uses Merkle set membership over all word hashes in the file. A verifier receives the Merkle root, which is deterministic over the full set of word hashes. This means: (a) the verifier learns the exact number of unique words in the file, and (b) the verifier can probe for any other word they suspect is present by computing its hash and checking it against the same root. That's "proof of knowledge" — it proves you know the content — but it's not zero-knowledge in the formal sense. A true ZK proof would let you prove keyword membership without revealing the root or the set size. The freshness proof is better — it's just a signed timestamp — but it relies entirely on trusting the filesystem's mtime, which is trivially forgeable.

What's still missing entirely:
The federated coordinator doesn't exist. The client side (federated push / federated pull) sends and receives JSON over HTTP, but there's nothing to send to. You need at minimum a lightweight coordinator process — even a simple FastAPI app that receives summaries from multiple agents, stores them, and serves the merged result on pull. Without this, the federated feature is a client with no server.
No unit tests for agmem itself. test_runner.py tests your agent's memory content, not the package. The system now has cryptographic signing, encryption, pack files, distributed locking, Merkle trees, differential privacy noise injection, and IPFS bundling — and none of it has automated test coverage. This is the single highest-risk gap in the project right now. A bug in retrieve_from_pack or _unbundle_objects could silently corrupt data and you'd have no way to catch it before it hits a user.
Pack file index uses linear scan. retrieve_from_pack iterates through every entry in the index sequentially until it finds the target hash. This is fine for repositories with hundreds of objects but becomes noticeably slow once you're in the tens of thousands. The index entries are already sorted by hash (because write_pack sorts hash_to_type.keys()), so this is a straightforward switch to binary search.
No delta encoding in pack files. Every object is stored in full. For semantic memory files that evolve incrementally over many commits — the most common pattern — you're storing the entire file content for each version. Git's delta encoding stores the first version in full and subsequent versions as the diff from a previous version, often achieving 5-10x compression on versioned text. This is the next level of pack file optimization.
Memory health monitoring is still shallow. The daemon does periodic Merkle verification. That's one check. It doesn't track storage growth rate, doesn't detect semantic redundancy across files, doesn't flag stale memory that hasn't been accessed, and doesn't check knowledge graph consistency (orphaned edges, contradictory facts without conflict markers). These are all things the daemon could do on its periodic tick without much added complexity.
GPU acceleration is still absent and remains low priority given the current scale of typical deployments.